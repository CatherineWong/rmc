{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import json \n",
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import analysis_utils\n",
    "import importlib \n",
    "from matplotlib.pylab import plt \n",
    "import importlib\n",
    "import analysis_utils\n",
    "from matplotlib.pylab import plt \n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import analysis_utils\n",
    "import random\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy.stats import wasserstein_distance_nd\n",
    "\n",
    "import marshaling.process_model as model_utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Load human data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' \n",
    "NOTE --- this may need to change\n",
    "Load in data, per query, per condition\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "''' \n",
    "Key other function from analysis_utils to change\n",
    "'''\n",
    "\n",
    "\n",
    "def load_and_process_samples_from_inference_results(inference_results_path, inference_file, n_queries):\n",
    "    with open(f\"{inference_results_path}{inference_file}\", \"r\") as f: \n",
    "        inf_data = json.load(f)\n",
    "        rollout_idx = int(inference_file.split(\"inference_results_\")[-1].split(\".json\")[0])\n",
    "        executability = inf_data['wm']['executability']\n",
    "        if executability == -1: return \n",
    "\n",
    "        # MCMC samples \n",
    "        # TODO ----- no longer 'model_metadata'\n",
    "        #full_samples = inf_data['wm']['model_metadata']['official_posterior_samples']\n",
    "        full_samples = inf_data['wm']['official_posterior_samples']   \n",
    "        # NEW\n",
    "        intermediate_samples = inf_data['wm']['intermediate_posterior_samples']   \n",
    "        \n",
    "        \n",
    "        # TODO ----- add option to load in per query ---- add in \"samples to process\"\n",
    "        # CHECK that this has the samples for the ablated versions?\n",
    "        \n",
    "        # Full synthesis model. This could include the ablated commentary variants later on.\n",
    "        samples_to_process = {\"full\" : full_samples}#, \"intermediate\": intermediate_samples}\n",
    "        # per condition\n",
    "        for condition_idx, cond_samples in enumerate(intermediate_samples): \n",
    "            samples_to_process[f'cond_{condition_idx}'] = cond_samples\n",
    "\n",
    "        processed_samples = {}\n",
    "        for samp_name, samples in samples_to_process.items():\n",
    "            if samples is None or len(samples) == 0: continue\n",
    "            if type(samples) == str: samples = eval(samples)\n",
    "            \n",
    "            # minimal processing of samples into query format\n",
    "            # {query1: [samples], query2: [samples]}\n",
    "            query_tags = set(samples[0][\"value\"].keys()) # all queries are computed per sample \n",
    "            if len(query_tags) != n_queries: \n",
    "                print(\"MISSING QUERIES: \", query_tags)\n",
    "                continue\n",
    "            \n",
    "            query_samples = {query: [] for query in query_tags}\n",
    "            for entry in samples: \n",
    "                for query, val in  entry[\"value\"].items():\n",
    "                    query_samples[query].append(val)\n",
    "                    \n",
    "            # process the values to make sure they are b/w 0-100\n",
    "            query_samples_processed = {query: [] for query in query_samples}\n",
    "            for query, values_list in query_samples.items(): \n",
    "                query_idx = int(query.split(\"query\")[-1])\n",
    "                if query_idx <= 8: \n",
    "                    if type(values_list[0]) == bool:\n",
    "                        print('wrong response form: ', query_idx, values_list[0])\n",
    "                        assert False\n",
    "                    elif np.max(values_list) <= 1: \n",
    "                        values_list = [v*100 for v in values_list]\n",
    "\n",
    "                query_samples_processed[query] = values_list    \n",
    "            processed_samples[samp_name] = query_samples_processed\n",
    "    return processed_samples\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "importlib.reload(analysis_utils)\n",
    "\n",
    "processed_vals_all_srcs =analysis_utils.process_values_from_all_sources(\n",
    "    all_scenarios=all_scenarios,\n",
    "    llm_res=llm_res,\n",
    "    baseline_human_data=baseline_human_data,\n",
    "    all_inference_data=all_inference_data,\n",
    "    gold_inference_data=all_gold_data,\n",
    "    n_queries=n_queries,\n",
    "    method=\"flat\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
